{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.distributed import destroy_process_group, init_process_group\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from model import GPT\n",
    "from trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read() + \"\\n\"\n",
    "\n",
    "\n",
    "def read_all_files_to_string(directory):\n",
    "    filepaths = [\n",
    "        filepath\n",
    "        for filepath in glob.glob(os.path.join(directory, \"**\", \"*\"), recursive=True)\n",
    "        if os.path.isfile(filepath)\n",
    "    ]\n",
    "\n",
    "    if not filepaths:\n",
    "        raise ValueError(\"No files found in the input directory.\")\n",
    "\n",
    "    combined_string = \"\"\n",
    "    with mp.Pool(min(len(filepaths), mp.cpu_count())) as executor:\n",
    "        results = executor.map(read_file, filepaths)\n",
    "        combined_string = \"\".join(results)\n",
    "\n",
    "    return combined_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def prepare_data(text: str, vocab_limit: int):\n",
    "    if not text:\n",
    "        raise ValueError(\n",
    "            \"The input text is empty. Please check the file reading process.\"\n",
    "        )\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = [line for line in lines if all(c.isascii() for c in line)]\n",
    "\n",
    "    if not lines:\n",
    "        raise ValueError(\"No valid ASCII lines found in the input text.\")\n",
    "\n",
    "    word_counts = Counter(word for line in lines[:100_000] for word in line.split())\n",
    "\n",
    "    most_common_words = [word for word, _ in word_counts.most_common(vocab_limit)]\n",
    "    words = most_common_words + [\"<unk>\", \"\\n\"]\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    stoi = {word: i for i, word in enumerate(words)}\n",
    "    itos = {i: word for i, word in enumerate(words)}\n",
    "\n",
    "    def encode(sentence):\n",
    "        tokens = [\n",
    "            stoi[word] if word in stoi else stoi[\"<unk>\"] for word in sentence.split()\n",
    "        ]\n",
    "        tokens.append(stoi[\"\\n\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(tokens):\n",
    "        words = [itos[token] for token in tokens]\n",
    "        return \" \".join(words).replace(\" \\n\", \"\\n\")\n",
    "\n",
    "    encoded_lines = [\n",
    "        torch.tensor(encode(line), dtype=torch.long) for line in lines if encode(line)\n",
    "    ]\n",
    "\n",
    "    if not encoded_lines:\n",
    "        raise ValueError(\"No lines were encoded. Check the encoding process.\")\n",
    "\n",
    "    return torch.cat(encoded_lines), encode, decode, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "block_size = 128\n",
    "max_epochs = 2000\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "save_interval = 10\n",
    "num_groups = 3\n",
    "vocab_limit = 10_000\n",
    "\n",
    "eval_str = r\"\"\"\n",
    "    Scene 1\n",
    "=======\n",
    "[Enter Theseus, Hippolyta, and Philostrate, with others.]\n",
    "\n",
    "\n",
    "THESEUS\n",
    "Now, fair Hippolyta, our nuptial hour\n",
    "Draws on apace. Four happy days bring in\n",
    "Another moon. But, O, methinks how slow\n",
    "This old moon wanes! She lingers my desires\n",
    "Like to a stepdame or a dowager\n",
    "Long withering out a young man's revenue.\n",
    "\n",
    "HIPPOLYTA\n",
    "Four days will quickly steep themselves in night;\n",
    "Four nights will quickly dream away the time;\n",
    "And then the moon, like to a silver bow\n",
    "New-bent in heaven, shall behold the night\n",
    "Of our solemnities.\n",
    "\n",
    "THESEUS  Go, Philostrate,\n",
    "Stir up the Athenian youth \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data\"\n",
    "snapshot_path = f\"{directory.replace('/', '_')}.pt\"\n",
    "\n",
    "text = read_all_files_to_string(directory)\n",
    "train_data, encode, decode, vocab_size = prepare_data(text, vocab_limit)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup():\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "    init_process_group(backend=\"nccl\")\n",
    "\n",
    "\n",
    "def load_train_objs():\n",
    "    train_dataset = TextDataset(train_data, block_size)\n",
    "\n",
    "    model = GPT(\n",
    "        vocab_size,\n",
    "        n_embd,\n",
    "        block_size,\n",
    "        n_layer,\n",
    "        n_head,\n",
    "        dropout,\n",
    "        num_groups,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return train_dataset, model, optimizer\n",
    "\n",
    "\n",
    "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        sampler=DistributedSampler(dataset),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_setup()\n",
    "dataset, model, optimizer = load_train_objs()\n",
    "train_loader = prepare_dataloader(dataset, batch_size)\n",
    "trainer = Trainer(model, train_loader, optimizer, save_interval, snapshot_path)\n",
    "trainer.train(max_epochs)\n",
    "destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    vocab_size,\n",
    "    n_embd,\n",
    "    block_size,\n",
    "    n_layer,\n",
    "    n_head,\n",
    "    dropout,\n",
    "    num_groups,\n",
    ").cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(snapshot_path)[\"MODEL_STATE\"])\n",
    "context = torch.tensor([encode(eval_str)], dtype=torch.long).cuda()\n",
    "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
